{"diagram":{"image":{"x":0,"width":200,"y":0,"pngdata":"iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAYAAACtWK6eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAsUlEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB8GXHmAAFMgHIEAAAAAElFTkSuQmCC","height":200},"elements":{"leftChildren":[],"children":[{"parent":"root","lineStyle":{"randomLineColor":"rgb(51, 156, 168)"},"children":[{"parent":"ac1321bdd87a","children":[{"parent":"00c5b25137f1","children":[{"parent":"7124cc6dd0e2","children":[],"id":"659da5e50f98","title":"高吞吐量（high&nbsp;throughput）来访问应用程序的数据，适合那些有着超大数据集的应用程序，这有可能会以延迟为代价。（低延迟看HBASE）。"},{"parent":"7124cc6dd0e2","children":[],"id":"25a59180ed3a","title":"高容错性（fault-tolerant），设计用来部署在低廉的硬件上。"},{"parent":"7124cc6dd0e2","children":[],"id":"590ee3113dae","title":"以流式数据访问模式存储超大文件而设计的文件系统。流式数据访问：一次写入、多次读取模式。"}],"id":"7124cc6dd0e2","title":"适用场景："},{"parent":"00c5b25137f1","children":[{"parent":"934b6e60378e","children":[],"id":"d47fe2f72c51","title":"大量的小文件：namenode存储着文件系统的元数据，文件数量的限制也由namenode的内存量决定。每个文件，索引目录以及块占大约150个字节，因此，如果有一百万文件，每个文件占一个块，就至少需要300MB的内存。"},{"parent":"934b6e60378e","children":[],"id":"ce494b16defc","title":"HDFS的块比磁盘的块大，目的是为了减少寻址的开销。"}],"id":"934b6e60378e","title":"一个理想的分片大小往往是一个HDFS block的大小，默认是64MB（可以通过配置文件指定）"}],"id":"00c5b25137f1","title":"HDFS (Hadoop&nbsp;Distributed&nbsp;File&nbsp;System：&nbsp;分布式文件存储)"},{"parent":"ac1321bdd87a","children":[{"parent":"9bd5320ffad9","children":[{"parent":"cd69d4f0b424","children":[],"id":"8c0c045380ba","title":"寻址时间长，慢于传输速率的发展历史。如果数据的访问模式中包含大量的磁盘寻址，比较适合以批处理的方式处理需要分析整个数据集，mapreduce流式读取hdfs块。"}],"id":"cd69d4f0b424","title":"存因"},{"parent":"9bd5320ffad9","children":[{"parent":"b8d2f5303784","children":[],"id":"20c8e849d11a","title":"一个基于集群的高性能并行计算平台（Cluster&nbsp;Infrastructure）"},{"parent":"b8d2f5303784","children":[{"parent":"b709753b3a8c","children":[],"id":"014b4d599cc3","title":"将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，运行在 yarn 集群"}],"id":"b709753b3a8c","title":"一个并行计算与运行软件框架（Software&nbsp;Framework）"},{"parent":"b8d2f5303784","children":[],"id":"f942ce7e7dfc","title":"一个并行程序设计模型与方法（Programming&nbsp;Model&nbsp;&amp;&nbsp;Methodology）"}],"id":"b8d2f5303784","title":"分布式体现"},{"parent":"9bd5320ffad9","children":[{"parent":"776c8b7ee68d","image":{"w":362,"h":218,"url":"http://cdn2.processon.com/607da344e4b0dcf1d14a7696?e=1618850133&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:Im0b9O6-qaf6qVSuPGkEzO2CxOg="},"children":[],"id":"fd7401a1ed0e","title":""},{"parent":"776c8b7ee68d","image":{"w":421,"h":318,"url":"http://cdn2.processon.com/607da4d0e4b0bc1d9eacd31f?e=1618850528&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:MvSP-daqq3h5Uqfa375KBKRW71g="},"children":[],"id":"2947b2b4113b","title":""}],"id":"776c8b7ee68d","title":"架构和执行过程"},{"parent":"9bd5320ffad9","children":[{"parent":"4e19af6405f0","children":[],"id":"3de331285e62","title":"数据流在走的时候，其实就是键值对的不断转换，把一个键值对变成另外一个键值对"},{"parent":"4e19af6405f0","children":[],"id":"d13523a39964","title":"MapReduce中的每个map任务可以细分2个阶段：record&nbsp;reader（input format类，决定对源文件的读取方式）、继承Mapper类（重写map方法）"},{"parent":"4e19af6405f0","children":[],"id":"c8d1481d365f","title":"shuffle阶段可以分为4步：分区（对数据打标记）、排序、规约（可选，减少网络数据的传输和拷贝）、分组（相同key放在同一个集合）"},{"parent":"4e19af6405f0","children":[],"id":"95c30408746a","title":"reduce任务可以分为2个阶段：排序与合并（编写reducer函数来实现逻辑，继承Reducer类，重写reduce逻辑）、选择输出格式（抽象成了OutputFormat类）"}],"id":"4e19af6405f0","title":"数据类型：&lt;key,value&gt;键值对"},{"parent":"9bd5320ffad9","children":[{"parent":"02a81a23ae9e","children":[{"parent":"2f7d939a2505","children":[],"id":"73088ec5722c","title":"在应用过程中，每一个MapReduce任务都有可能出错，都需要重试和异常处理的机制。<br>协调这些子MapReduce的任务往往需要和业务逻辑紧密耦合的状态机，过于复杂的维护让系统开发者苦不堪言。"}],"id":"2f7d939a2505","title":"1.&nbsp;高昂的维护成本"},{"parent":"02a81a23ae9e","children":[{"parent":"abe118782496","children":[{"parent":"585d31792a9a","children":[],"id":"141acfc6e688","title":"应对: google的动态分片技术"}],"id":"585d31792a9a","title":"在MapReduce的性能配置上花了非常多的时间。包括了缓冲大小(buffer&nbsp;size），分片多少（number&nbsp;of&nbsp;shards），预抓取策略（prefetch），缓存大小（cache&nbsp;size）等等"}],"id":"abe118782496","title":"2. 复杂的性能优化配置"}],"id":"02a81a23ae9e","title":"被逐渐淘汰原因"}],"id":"9bd5320ffad9","title":"MapReduce"},{"parent":"ac1321bdd87a","children":[{"parent":"0e340348cf1d","children":[{"parent":"51945cf1427f","children":[],"id":"cfd1eb52982b","title":"Hadoop1.0&nbsp;中架构的缺陷，在&nbsp;MapReduce&nbsp;中，jobTracker&nbsp;担负起了太多的责任了，接收任务是它，资源调度是它，监控&nbsp;TaskTracker&nbsp;运行情况还是它。将&nbsp;jobTracker&nbsp;进行拆分，将其中部分功能拆解出来"}],"id":"51945cf1427f","title":"Yet&nbsp;Another&nbsp;Resource&nbsp;Negotiator: 分布式资源调度系统"},{"parent":"0e340348cf1d","children":[{"parent":"43a03d42f23c","children":[{"parent":"ac96df121ae3","children":[],"id":"3727242fcf16","title":"整个计算任务资源的调度和分配"}],"id":"ac96df121ae3","title":"主节点：ResourceManager"},{"parent":"43a03d42f23c","children":[{"parent":"20652ccee7db","children":[],"id":"88a14954475f","title":"执行具体的运算"}],"id":"20652ccee7db","title":"从节点：NodeManager"}],"id":"43a03d42f23c","title":"主从架构"},{"parent":"0e340348cf1d","image":{"w":691,"h":351,"url":"http://cdn2.processon.com/607da18de4b0f1528e420713?e=1618849694&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:cSoHsMX5KiHok3rh70VTWUYDRP0="},"children":[],"id":"02c45029802e","title":"提交一个程序所经历的简单流程：<br>1. Client&nbsp;向&nbsp;Yarn&nbsp;提交&nbsp;Application，这里我们假设是一个&nbsp;MapReduce&nbsp;作业。<br>2. ResourceManager&nbsp;向&nbsp;NodeManager&nbsp;通信，为该&nbsp;Application&nbsp;分配第一个容器。并在这个容器中运行这个应用程序对应的&nbsp;ApplicationMaster。<br>3. ApplicationMaster&nbsp;启动以后，对&nbsp;作业（也就是&nbsp;Application）&nbsp;进行拆分，拆分&nbsp;task&nbsp;出来，这些&nbsp;task&nbsp;可以运行在一个或多个容器中。然后向&nbsp;ResourceManager&nbsp;申请要运行程序的容器，并定时向&nbsp;ResourceManager&nbsp;发送心跳。<br>4. 申请到容器后，ApplicationMaster&nbsp;会去和容器对应的&nbsp;NodeManager&nbsp;通信，而后将作业分发到对应的&nbsp;NodeManager&nbsp;中的容器去运行，这里会将拆分后的&nbsp;MapReduce&nbsp;进行分发，对应容器中运行的可能是&nbsp;Map&nbsp;任务，也可能是&nbsp;Reduce&nbsp;任务。<br>5. 容器中运行的任务会向&nbsp;ApplicationMaster&nbsp;发送心跳，汇报自身情况。当程序运行完成后，&nbsp;ApplicationMaster&nbsp;再向&nbsp;ResourceManager&nbsp;注销并释放容器资源。<br>"}],"id":"0e340348cf1d","title":"Yarn"}],"id":"ac1321bdd87a","title":"1. Hadoop概述"},{"parent":"root","lineStyle":{"randomLineColor":"rgb(232, 124, 37)"},"children":[{"parent":"84d1cdcf6f18","children":[],"id":"5b1fe40e36b3","title":"添加打包方式，如&lt;packaging&gt;jar&lt;/packaging&gt;"},{"parent":"84d1cdcf6f18","children":[{"parent":"faa1c06621d4","children":[],"id":"805a0f1a1b64","title":"继承InputFormat, 用对应的接口，或者改写为自己的方法"},{"parent":"faa1c06621d4","children":[{"parent":"cf401253873e","image":{"w":422,"h":198,"url":"http://cdn2.processon.com/607f9beee4b0bc1d9eaff29b?e=1618979326&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:mCY1315q-_qb-m3qiYmMKUzdWjc="},"children":[],"id":"d05f8b78e73c","title":"样例"}],"id":"cf401253873e","title":"继承Mapper，重写map方法"}],"id":"faa1c06621d4","title":"map"},{"parent":"84d1cdcf6f18","children":[{"parent":"893a584065da","children":[{"parent":"d83f29297077","children":[],"id":"beb34995dbf4","title":"也即给数据打一个标记，通过指定分区，将同一个分区的数据发送到同一个reduce当中进行处理"},{"parent":"d83f29297077","image":{"w":479,"h":168,"url":"http://cdn2.processon.com/607f9a4be4b0bc1d9eafec81?e=1618978907&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:6gg_82s5K_Ga8IcUycRjeKxJh-E="},"children":[],"id":"f99e50974002","title":""},{"parent":"d83f29297077","image":{"w":275,"h":277,"url":"http://cdn2.processon.com/607f9cc1e4b0c5be8ad3f053?e=1618979537&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:JpLaHhLwq9bgheg5LWN4FEq_pUc="},"children":[],"id":"e2667f8ca467","title":"代码样例"},{"parent":"d83f29297077","children":[{"parent":"c71fdd46579a","image":{"w":574,"h":125,"url":"http://cdn2.processon.com/607f9d0ae4b0dcf1d14da172?e=1618979610&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:D6BJtGMxWYul94UzezPYBhaXJvs="},"children":[],"id":"d1717d06ebef","title":"样例"}],"id":"c71fdd46579a","title":"在主类中设置分区类和ReduceTask个数"}],"id":"d83f29297077","title":"分区"},{"parent":"893a584065da","children":[{"parent":"e52e93e11f82","children":[],"id":"d561617a6c7e","title":"Java&nbsp;的序列化&nbsp;(Serializable)&nbsp;是一个重量级序列化框架,&nbsp;一个对象被序列化后,&nbsp;会附带很多额<br>外的信息&nbsp;(各种校验信息,&nbsp;header,&nbsp;继承体系等）,&nbsp;不便于在网络中高效传输.&nbsp;所以,&nbsp;Hadoop&nbsp;<br>自己开发了一套序列化机制(Writable),&nbsp;精简高效.&nbsp;不用像&nbsp;Java&nbsp;对象类一样传输多层的父子<br>关系,&nbsp;需要哪个属性就传输哪个属性值,&nbsp;大大的减少网络传输的开销"},{"parent":"e52e93e11f82","children":[],"id":"7e3a62303be1","title":"Writable&nbsp;是&nbsp;Hadoop&nbsp;的序列化格式,&nbsp;Hadoop&nbsp;定义了这样一个&nbsp;Writable&nbsp;接口.&nbsp;一个类要支持可<br>序列化只需实现这个接口即可"},{"parent":"e52e93e11f82","children":[],"id":"2aad937bb4d6","title":"快排法"}],"id":"e52e93e11f82","title":"排序和序列化"},{"parent":"893a584065da","children":[{"parent":"486bc42eca4b","image":{"w":401,"h":172,"url":"http://cdn2.processon.com/607f9972e4b0d94bcabcd36d?e=1618978690&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:eBCxnlx1Pj-FZpPRJ56BAsqiyXw="},"children":[],"id":"26c08b0eabf0","title":"每一个&nbsp;map&nbsp;都可能会产生大量的本地输出，Combiner&nbsp;的作用就是对&nbsp;map&nbsp;端的输出先做一次<br>合并，以减少在&nbsp;map&nbsp;和&nbsp;reduce&nbsp;节点之间的数据传输量，以提高网络IO&nbsp;性能，是&nbsp;MapReduce<br>的一种优化手段之一"},{"parent":"486bc42eca4b","children":[],"id":"46927a80a12a","title":"combiner&nbsp;是&nbsp;MR&nbsp;程序中&nbsp;Mapper&nbsp;和&nbsp;Reducer&nbsp;之外的一种组件，combiner&nbsp;组件的父类就是&nbsp;Reducer，combiner&nbsp;和&nbsp;reducer&nbsp;的区别在于运行的位置，Combiner&nbsp;是在每一个&nbsp;maptask&nbsp;所在的节点运行，Reducer&nbsp;是接收全局所有&nbsp;Mapper&nbsp;的输出结果"},{"parent":"486bc42eca4b","children":[{"parent":"484ecc70884f","children":[],"id":"ec11aa9c275a","title":"1.自定义一个&nbsp;combiner&nbsp;继承&nbsp;Reducer，重写&nbsp;reduce&nbsp;方法<br>2.&nbsp;在&nbsp;job&nbsp;中设置&nbsp;job.setCombinerClass(CustomCombiner.class)<br>3. combiner&nbsp;能够应用的前提是不能影响最终的业务逻辑，而且，combiner&nbsp;的输出&nbsp;kv&nbsp;应该跟<br>reducer&nbsp;的输入&nbsp;kv&nbsp;类型要对应起来"}],"id":"484ecc70884f","title":"实现步骤"},{"parent":"486bc42eca4b","children":[],"id":"2972d860dd9f","title":"样例：手机流量统计"}],"id":"486bc42eca4b","title":"规约"},{"parent":"893a584065da","children":[],"id":"997c078cd0d1","title":"分组"}],"id":"893a584065da","title":"shuffle"},{"parent":"84d1cdcf6f18","children":[{"parent":"c35ad7106b44","children":[{"parent":"c7fe77d78d42","image":{"w":312,"h":256,"url":"http://cdn2.processon.com/607f9c28e4b0c5be8ad3ee78?e=1618979384&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:XCJxffVtv4N4o-6Lt6gYFXQP5mg="},"children":[],"id":"58b308688362","title":"样例"}],"id":"c7fe77d78d42","title":"继承Reducer，重写reduce方法"},{"parent":"c35ad7106b44","children":[],"id":"adc4278f125f","title":"继承OutputFormat,&nbsp;用对应的接口，或者改写为自己的方法"}],"id":"c35ad7106b44","title":"reduce"},{"parent":"84d1cdcf6f18","children":[{"parent":"e55235554798","children":[],"id":"65742fcc61a7","title":"extends Configured implements Tool"},{"parent":"e55235554798","children":[{"parent":"a5212561bbac","children":[],"id":"12c19af0d804","title":"创建job任务对象（Job.getInstance）"},{"parent":"a5212561bbac","children":[{"parent":"c4d35b50f4c2","children":[{"parent":"2dcf6351dae8","children":[],"id":"508b2b88d731","title":"job.setInputFortmatClass"},{"parent":"2dcf6351dae8","children":[],"id":"d81c0e255f57","title":"TextInputFormat.addInputPath"}],"id":"2dcf6351dae8","title":"1. 指定文件的读取方式和读取路径"},{"parent":"c4d35b50f4c2","children":[{"parent":"c64af8471739","children":[],"id":"9fa04ac3a0bc","title":"job.setMapperClass"},{"parent":"c64af8471739","children":[{"parent":"f2a5f173633e","children":[],"id":"a4fee2796b81","title":"job.setMapOutKeyClass"}],"id":"f2a5f173633e","title":"设置Map阶段K2的类型"},{"parent":"c64af8471739","children":[{"parent":"1e6aaf529ad4","children":[],"id":"373e78d1fc9b","title":"job.setMapOutputValueClass"}],"id":"1e6aaf529ad4","title":"设置Map阶段V2的类型"}],"id":"c64af8471739","title":"2. 指定Map阶段的处理方式"},{"parent":"c4d35b50f4c2","children":[],"id":"946f8f7cbdf3","title":"3. 分区"},{"parent":"c4d35b50f4c2","children":[],"id":"36bb07229957","title":"4. 排序"},{"parent":"c4d35b50f4c2","children":[],"id":"1b06d7309b2a","title":"5. 规约"},{"parent":"c4d35b50f4c2","children":[],"id":"b6ad907bb527","title":"6. 分组"},{"parent":"c4d35b50f4c2","children":[{"parent":"13159d702029","children":[],"id":"4c6493374c00","title":"job.setReducerClass"},{"parent":"13159d702029","children":[{"parent":"6f43e105e125","children":[],"id":"8897b08181ce","title":"job.setOuputKeyClass"}],"id":"6f43e105e125","title":"设置Reduce阶段K3的类型"},{"parent":"13159d702029","children":[{"parent":"f3ef88fb86a1","children":[],"id":"df2b4a18c3da","title":"job.setOutputValueClass"}],"id":"f3ef88fb86a1","title":"设置Reduce阶段V3的类型"}],"id":"13159d702029","title":"7. 指定reduce阶段的处理方式和数据类型"},{"parent":"c4d35b50f4c2","children":[{"parent":"1ef9bd140078","children":[],"id":"271efacf07d9","title":"job.setOutputFortmatClass"},{"parent":"1ef9bd140078","children":[],"id":"6d6300be096c","title":"TextOutputFormat.setOutputPath"}],"id":"1ef9bd140078","title":"8. 设置输出类型"},{"parent":"c4d35b50f4c2","children":[{"parent":"646e3a356837","children":[],"id":"7a0ceb89a414","title":"boolean bl = job.waitForCompletion(true)&nbsp; return bl?0:1"}],"id":"646e3a356837","title":"等待任务结束"}],"id":"c4d35b50f4c2","title":"配置job任务对象（八个步骤）"}],"id":"a5212561bbac","title":"重写run方法"},{"parent":"e55235554798","children":[{"parent":"34b242cb791f","children":[{"parent":"ade58fb39daf","image":{"w":362,"h":186,"url":"http://cdn2.processon.com/607f9c87e4b0c5be8ad3efe7?e=1618979479&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:FxlkwRLkxvaHY8ImXPZ8j_G8E3o="},"children":[],"id":"8525223cb9e6","title":"样例"}],"collapsed":false,"id":"ade58fb39daf","title":"启动job任务（ToolRunner.run），入口"}],"collapsed":false,"id":"34b242cb791f","title":"main"}],"id":"e55235554798","title":"主类代码 (将八个步骤串到一起)"},{"parent":"84d1cdcf6f18","children":[{"parent":"91dda261c98c","children":[{"parent":"a5db32679010","children":[],"id":"f8020e201e6c","title":"1. 将mapreduce程序交给yarn集群，分发到很多节点上并发执行"},{"parent":"a5db32679010","children":[],"id":"2bdfbd4826b8","title":"2. 处理的数据和输出结果位于HDFS文件系统"},{"parent":"a5db32679010","children":[{"parent":"521f3d9de951","children":[],"id":"8889d3033c4d","title":"hadoop&nbsp; &nbsp;jar&nbsp; &nbsp; jar包名字.jar&nbsp; &nbsp;jar包中主方法的全路径名"}],"id":"521f3d9de951","title":"3. 实现步骤： 将程序打成JAR包，并上传，然后在集群上用hadoop命令启动"}],"id":"a5db32679010","title":"集群运行模式"},{"parent":"91dda261c98c","children":[{"parent":"5f3068d04390","image":{"w":302,"h":156,"url":"http://cdn2.processon.com/607f98cfe4b07192168b1855?e=1618978527&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:YXZ9YZlPtYN9R7aGrr4ap06ssdU="},"children":[],"id":"ebe12dcaecbe","title":""}],"id":"5f3068d04390","title":"本地运行模式"}],"id":"91dda261c98c","title":"运行"}],"id":"84d1cdcf6f18","title":"2. 流程详解"},{"parent":"root","lineStyle":{"randomLineColor":"#e85d4e"},"children":[{"parent":"cdce42bd6a07","children":[{"parent":"4b8f374d4da6","children":[],"id":"a76bfed912f4","title":"1.&nbsp;读取数据组件&nbsp;InputFormat&nbsp;(默认&nbsp;TextInputFormat)&nbsp;会通过&nbsp;getSplits&nbsp;方法对输入目录<br>中文件进行逻辑切片规划得到&nbsp;block&nbsp;,&nbsp;有多少个&nbsp;block&nbsp;就对应启动多少个&nbsp;MapTask&nbsp;."},{"parent":"4b8f374d4da6","children":[],"id":"4806dde0dcb0","title":"2.&nbsp;将输入文件切分为&nbsp;block&nbsp;之后,&nbsp;由&nbsp;RecordReader&nbsp;对象&nbsp;(默认是LineRecordReader)&nbsp;进&nbsp;行读取,&nbsp;以&nbsp;\\n&nbsp;作为分隔符,&nbsp;读取一行数据,&nbsp;返回&nbsp;&lt;key，value&gt;&nbsp;.&nbsp;Key&nbsp;表示每行首字符偏<br>移值,&nbsp;Value&nbsp;表示这一行文本内容"},{"parent":"4b8f374d4da6","children":[],"id":"35d7e17e5ba1","title":"3. 读取&nbsp;block&nbsp;返回&nbsp;&lt;key,value&gt;&nbsp;,&nbsp;进入用户自己继承的&nbsp;Mapper&nbsp;类中，执行用户重写<br>的&nbsp;map&nbsp;函数,&nbsp;RecordReader&nbsp;读取一行这里调用一次"},{"parent":"4b8f374d4da6","children":[],"id":"02bae1e47e89","title":"4. Mapper&nbsp;逻辑结束之后,&nbsp;将&nbsp;Mapper&nbsp;的每条结果通过&nbsp;context.write&nbsp;进行collect数据收<br>集.&nbsp;在&nbsp;collect&nbsp;中,&nbsp;会先对其进行分区处理，默认使用&nbsp;HashPartitioner"},{"parent":"4b8f374d4da6","children":[],"id":"ee788c916059","title":"5. 接下来,&nbsp;会将数据写入内存,&nbsp;内存中这片区域叫做环形缓冲区,&nbsp;缓冲区的作用是批量收集<br>Mapper&nbsp;结果,&nbsp;减少磁盘&nbsp;IO&nbsp;的影响.&nbsp;我们的&nbsp;Key/Value&nbsp;对以及&nbsp;Partition&nbsp;的结果都会被写入<br>缓冲区.&nbsp;当然,&nbsp;写入之前，Key&nbsp;与&nbsp;Value&nbsp;值都会被序列化成字节数组"},{"parent":"4b8f374d4da6","children":[],"id":"4c62797c80fd","title":"6. 当溢写线程启动后,&nbsp;需要对这&nbsp;80MB&nbsp;空间内的&nbsp;Key&nbsp;做排序&nbsp;(Sort).&nbsp;排序是&nbsp;MapReduce&nbsp;模型<br>默认的行为,&nbsp;这里的排序也是对序列化的字节做的排序"},{"parent":"4b8f374d4da6","children":[],"id":"8a0dc73f7506","title":"7. 合并溢写文件,&nbsp;每次溢写会在磁盘上生成一个临时文件&nbsp;(写之前判断是否有&nbsp;Combiner),&nbsp;如&nbsp;果&nbsp;Mapper&nbsp;的输出结果真的很大,&nbsp;有多次这样的溢写发生,&nbsp;磁盘上相应的就会有多个临时文<br>件存在.&nbsp;当整个数据处理结束之后开始对磁盘中的临时文件进行&nbsp;Merge&nbsp;合并,&nbsp;因为最终的<br>文件只有一个,&nbsp;写入磁盘,&nbsp;并且为这个文件提供了一个索引文件,&nbsp;以记录每个reduce对应数<br>据的偏移量"}],"id":"4b8f374d4da6","title":"inputFile通过split被逻辑切分为多个split文件，通过Record按行读取内容给<br>map（用户自己实现的）进行处理，数据被map处理结束之后交给OutputCollector收集器，对<br>其结果key进行分区（默认使用hash分区），然后写入buwer，每个map&nbsp;task都有一个内存缓冲<br>区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方<br>式存放到磁盘，当整个map&nbsp;task结束后再对磁盘中这个map&nbsp;task产生的所有临时文件做合并，<br>生成最终的正式输出文件，然后等待reduce&nbsp;task来拉数据"}],"id":"cdce42bd6a07","title":"Map详细机制"},{"parent":"root","lineStyle":{"randomLineColor":"rgb(255,204,204)"},"children":[{"parent":"aa32d8bc8e22","children":[{"parent":"9d325fd4992b","children":[],"id":"2535c94f890c","title":"1. Copy阶段&nbsp;，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP<br>方式请求maptask获取属于自己的文件。<br>2.&nbsp;Merge阶段&nbsp;。这里的merge如map端的merge动作，只是数组中存放的是不同map端<br>copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端<br>的更为灵活。merge有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第<br>一种形式不启用。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map&nbsp;<br>端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后<br>在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据<br>时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件。<br>3.&nbsp;合并排序&nbsp;。把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。<br>4.&nbsp;对排序后的键值对调用reduce方法&nbsp;，键相等的键值对调用一次reduce方法，每次调用会<br>产生零个或者多个键值对，最后把这些输出的键值对写入到HDFS文件中。"}],"id":"9d325fd4992b","title":"Reduce&nbsp;大致分为&nbsp;copy、sort、reduce&nbsp;三个阶段，重点在前两个阶段。copy&nbsp;阶段包含一个<br>eventFetcher&nbsp;来获取已完成的&nbsp;map&nbsp;列表，由&nbsp;Fetcher&nbsp;线程去&nbsp;copy&nbsp;数据，在此过程中会启动两<br>个&nbsp;merge&nbsp;线程，分别为&nbsp;inMemoryMerger&nbsp;和&nbsp;onDiskMerger，分别将内存中的数据&nbsp;merge&nbsp;到磁<br>盘和将磁盘中的数据进行&nbsp;merge。待数据&nbsp;copy&nbsp;完成之后，copy&nbsp;阶段就完成了，开始进行&nbsp;sort阶段，sort&nbsp;阶段主要是执行&nbsp;finalMerge&nbsp;操作，纯粹的&nbsp;sort&nbsp;阶段，完成之后就是&nbsp;reduce&nbsp;阶段，<br>调用用户定义的&nbsp;reduce&nbsp;函数进行处理"}],"id":"aa32d8bc8e22","title":"Reduce详细机制"},{"parent":"root","lineStyle":{"randomLineColor":"#e85d4e"},"children":[{"parent":"7dbc95fa9f79","children":[],"id":"5c9ccac3fe56","title":"辅助诊断系统故障"},{"parent":"7dbc95fa9f79","children":[{"parent":"136a51faedbe","image":{"w":436,"h":190,"url":"http://cdn2.processon.com/607f9d5ae4b0bc1d9eaffb87?e=1618979690&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:mjLgII-UhbLmek_wZ-mWk1lGA3I="},"children":[],"id":"eb7f040449a8","title":"样例代码"}],"id":"136a51faedbe","title":"1. 通过context上下文对象可以获取我们的计数器<br>"},{"parent":"7dbc95fa9f79","children":[{"parent":"986a245c1cf5","image":{"w":446,"h":192,"url":"http://cdn2.processon.com/607f9d7be4b0bc1d9eaffbbf?e=1618979723&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:ln8SWIKieWN0undpz4Ligfpf3DQ="},"children":[],"id":"53b4e6e3ecaf","title":"样例代码"}],"id":"986a245c1cf5","title":"2.&nbsp;通过enum枚举类型来定义计数器"}],"id":"7dbc95fa9f79","title":"计数器"},{"parent":"root","children":[{"parent":"9c9047803e79","children":[{"parent":"ff77f8d17ff8","children":[],"id":"cfd345711da5","title":"子主题"}],"id":"ff77f8d17ff8","title":"1. 词频统计wordcount"},{"parent":"9c9047803e79","children":[],"id":"ef94a68d1489","title":"2. 手机流量统计、求和"},{"parent":"9c9047803e79","children":[{"parent":"68abd8b375cf","children":[{"parent":"bf4718efc88f","image":{"w":791,"h":244,"url":"http://cdn2.processon.com/607f9ab6e4b08d5f4337c9ed?e=1618979014&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:62H1X6ISAoywUcGe8_A408hyFu8="},"children":[],"id":"b07600f7091a","title":""}],"id":"bf4718efc88f","title":"map端"},{"parent":"68abd8b375cf","children":[{"parent":"ea038d3172f2","image":{"w":708,"h":254,"url":"http://cdn2.processon.com/607f9af1e4b0e306bc743729?e=1618979073&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:WDJurWTv9NSV9qsCgEoZow3i7vA="},"children":[],"id":"c37847cd6c41","title":""},{"parent":"ea038d3172f2","children":[{"parent":"335659797d3b","image":{"w":733,"h":128,"url":"http://cdn2.processon.com/607f9b0de4b0bc1d9eaff127?e=1618979101&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:_h4eLv-b_ODprqNECEBU-DEJMlE="},"children":[],"id":"6f1b224a2bc0","title":""}],"id":"335659797d3b","title":"存在的问题"}],"id":"ea038d3172f2","title":"reduce端"}],"id":"68abd8b375cf","title":"3. 实现join操作"},{"parent":"9c9047803e79","children":[{"parent":"e6b40247d0c6","image":{"w":900,"h":170.22767075306479,"url":"http://cdn2.processon.com/607f9a8ce4b0dcf1d14d9c79?e=1618978972&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:m4BFbW_UiPXVmWkFpmARTee2TpQ="},"children":[],"id":"2fc1b66ce068","title":""}],"id":"e6b40247d0c6","title":"4. 求共同好友"},{"parent":"9c9047803e79","children":[{"parent":"5927e337a61a","image":{"w":662,"h":150,"url":"http://cdn2.processon.com/607f9b3ee4b0e306bc7437c7?e=1618979150&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:8CIbOWRfdUfek1T2TsKIUxsNMkY="},"children":[],"id":"c3e5b34686dd","title":"自定义inputformat"}],"id":"5927e337a61a","title":"5. 小文件合并"},{"parent":"9c9047803e79","children":[{"parent":"8d2e5f6cd3b5","children":[{"parent":"baa293b08bc4","image":{"w":900,"h":242.12283044058745,"url":"http://cdn2.processon.com/607f9b88e4b08d5f4337cbbc?e=1618979224&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:vtuCdaqjvTzl6Dl-bYVFTuC60_I="},"children":[],"id":"4e403a6ad967","title":"自定义分组"}],"id":"baa293b08bc4","title":"自定义分组"}],"id":"8d2e5f6cd3b5","title":"6.求TopN"},{"parent":"9c9047803e79","children":[{"parent":"d64ee004546b","image":{"w":900,"h":137.66626360338574,"url":"http://cdn2.processon.com/607f9b6de4b0fec2611bd8f9?e=1618979197&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:pS5lRZJSuNOOI6oD_hSlLjw68y8="},"children":[],"id":"3e764920f33e","title":""}],"id":"d64ee004546b","title":"7. 自动逸outpuutformat"}],"id":"9c9047803e79","title":"综合案例"},{"parent":"root","children":[{"parent":"4a00ef279536","children":[],"id":"f2b631c733fb","title":"1. 结果是否缓存下来"},{"parent":"4a00ef279536","children":[],"id":"5d2b8ac69dd0","title":"2. 寻址的需求，一次写入，多次读取"}],"id":"4a00ef279536","title":"和oracle分区的相似与不同之处"},{"parent":"root","children":[{"parent":"64b6b8f7da64","children":[],"id":"fdb18ebe7203","title":"1.&nbsp;Collect阶段&nbsp;：将&nbsp;MapTask&nbsp;的结果输出到默认大小为&nbsp;100M&nbsp;的环形缓冲区，保存的是<br>key/value，Partition&nbsp;分区信息等。<br>2.&nbsp;Spill阶段&nbsp;：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，<br>在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了&nbsp;combiner，还会将<br>有相同分区号和&nbsp;key&nbsp;的数据进行排序。<br>3.&nbsp;Merge阶段&nbsp;：把所有溢出的临时文件进行一次合并操作，以确保一个&nbsp;MapTask&nbsp;最终只<br>产生一个中间数据文件。<br>4.&nbsp;Copy阶段&nbsp;：ReduceTask&nbsp;启动&nbsp;Fetcher&nbsp;线程到已经完成&nbsp;MapTask&nbsp;的节点上复制一份属于<br>自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阀值<br>的时候，就会将数据写到磁盘之上。<br>5.&nbsp;Merge阶段&nbsp;：在&nbsp;ReduceTask&nbsp;远程复制数据的同时，会在后台开启两个线程对内存到本<br>地的数据文件进行合并操作。<br>6.&nbsp;Sort阶段&nbsp;：在对数据进行合并的同时，会进行排序操作，由于&nbsp;MapTask&nbsp;阶段已经对数<br>据进行了局部的排序，ReduceTask&nbsp;只需保证&nbsp;Copy&nbsp;的数据的最终整体有效性即可。<br>Shuwle&nbsp;中的缓冲区大小会影响到&nbsp;mapreduce&nbsp;程序的执行效率，原则上说，缓冲区越大，<br>磁盘io的次数越少，执行速度就越快<br>缓冲区的大小可以通过参数调整,&nbsp;参数：mapreduce.task.io.sort.mb&nbsp;默认100M"}],"id":"64b6b8f7da64","title":"shuffle详细机制"},{"parent":"root","image":{"w":900,"h":407.2282299373933,"url":"http://cdn2.processon.com/6080ff71e4b0e306bc76bb89?e=1619070337&token=trhI0BY8QfVrIGn9nENop6JAc6l5nZuxhjQ62UfM:1AtSUrESYux_j4YJhPFxQRD7V2s="},"children":[],"id":"e0e07edf31c1","title":"总结"}],"root":true,"theme":"colorLines","id":"root","title":"MapReduce","lines":{},"structure":"mind_right"}},"meta":{"exportTime":"2021-05-10 19:24:55","member":"","diagramInfo":{"creator":"","created":"","modified":"","title":"","category":""},"id":"","type":"ProcessOn Schema File","version":"1.0"}}